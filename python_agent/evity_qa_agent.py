#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Evity QA Agent â€“ versiÃ³n bilingÃ¼e (ES/EN) con reconstrucciÃ³n automÃ¡tica del Ã­ndice.
- Lee .txt y .pdf de ./contenidos
- Detecta idioma y traduce al espaÃ±ol si hace falta (para uniformar la bÃºsqueda)
- Genera embeddings y guarda un Ã­ndice local
- Si agregas/actualizas archivos, se reconstruye automÃ¡ticamente al preguntar
- Responde con un tono empÃ¡tico y comprensible para pacientes
- PersonalizaciÃ³n: usa nombre de la persona usuaria, responde saludos y agradecimientos
"""

import argparse
import os
import re
import time
from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np
from openai import OpenAI
from pypdf import PdfReader
from tqdm import tqdm

# ---------------------------------------------------------------------------
# Utilidades de lectura
# ---------------------------------------------------------------------------


def _read_txt(p: Path) -> str:
    return p.read_text(encoding="utf-8", errors="ignore")


def _read_pdf(p: Path) -> str:
    """Extrae texto de un PDF usando pypdf."""
    text_parts = []
    try:
        reader = PdfReader(str(p))
        for page in reader.pages:
            txt = page.extract_text() or ""
            if txt:
                text_parts.append(txt)
    except Exception as e:
        print(f"[pdf] âš ï¸ No pude leer {p.name}: {e}")
    text = "\n".join(text_parts)
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text


def collect_documents(contenidos_dir: Path) -> List[Tuple[str, str]]:
    """Lee .txt y .pdf dentro de la carpeta `contenidos/` y devuelve [(nombre, texto)]."""
    docs: List[Tuple[str, str]] = []
    if not contenidos_dir.exists():
        print(f"[warn] No existe carpeta: {contenidos_dir}")
        return docs

    # .txt
    for p in sorted(contenidos_dir.glob("*.txt")):
        try:
            txt = _read_txt(p)
            if txt.strip():
                docs.append((p.name, txt))
        except Exception as e:
            print(f"[txt] âš ï¸ Error leyendo {p.name}: {e}")

    # .pdf
    for p in sorted(contenidos_dir.glob("*.pdf")):
        try:
            txt = _read_pdf(p)
            if txt.strip():
                docs.append((p.name, txt))
            else:
                print(f"[pdf] âš ï¸ PDF vacÃ­o o sin texto: {p.name}")
        except Exception as e:
            print(f"[pdf] âš ï¸ Error leyendo {p.name}: {e}")

    print(f"[info] Documentos cargados desde {contenidos_dir}: {len(docs)}")
    return docs


# ---------------------------------------------------------------------------
# Embeddings y helpers
# ---------------------------------------------------------------------------


def detect_and_translate(client: OpenAI, text: str) -> str:
    """
    Detecta idioma y traduce al espaÃ±ol si el texto estÃ¡ en inglÃ©s (usa un fragmento).
    Si ya estÃ¡ en espaÃ±ol, devuelve el texto sin cambios.
    """
    try:
        frag = text[:3000]  # suficiente para decidir idioma y traducir gist
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Detecta el idioma del mensaje del usuario. "
                        "Si estÃ¡ en inglÃ©s, tradÃºcelo al espaÃ±ol conservando el significado. "
                        "Si ya estÃ¡ en espaÃ±ol, regrÃ©salo igual. "
                        "Devuelve solo el texto resultante, sin comentarios."
                    ),
                },
                {"role": "user", "content": frag},
            ],
            temperature=0.0,
        )
        return resp.choices[0].message.content.strip() or text
    except Exception as e:
        print(f"[trans] âš ï¸ Error traduciendo: {e}")
        return text


def embed_texts_openai(
    client: OpenAI, texts: List[str], model: str = "text-embedding-3-small"
) -> np.ndarray:
    """Crea embeddings con OpenAI en batches."""
    batch_size = 50
    all_vecs: List[List[float]] = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Creando embeddings"):
        batch = texts[i : i + batch_size]
        resp = client.embeddings.create(model=model, input=batch)
        vecs = [d.embedding for d in resp.data]
        all_vecs.extend(vecs)
    return np.array(all_vecs, dtype=np.float32)


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12))


def search_similar(query_emb: np.ndarray, index_embs: np.ndarray, k: int = 5):
    sims = [cosine_similarity(query_emb, emb) for emb in index_embs]
    top_k = np.argsort(sims)[::-1][:k]
    return top_k, [sims[i] for i in top_k]


# ---------------------------------------------------------------------------
# ConstrucciÃ³n del Ã­ndice
# ---------------------------------------------------------------------------


def build_index(base: Path):
    contenidos_dir = base / "contenidos"
    out_dir = base / "vector_index"
    out_dir.mkdir(parents=True, exist_ok=True)

    docs = collect_documents(contenidos_dir)
    if not docs:
        print("[warn] No hay documentos para indexar.")
        return

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))

    names: List[str] = []
    texts: List[str] = []

    print("\nðŸ§  Detectando idioma y traduciendo si es necesario...\n")
    for name, text in tqdm(docs):
        translated = detect_and_translate(client, text)
        texts.append(translated)
        names.append(name)

    embs = embed_texts_openai(client, texts)

    # Guardamos todo lo necesario para reconstruir el contexto sin leer otra vez
    np.savez(
        out_dir / "index_evity.npz",
        names=np.array(names, dtype=object),
        texts=np.array(texts, dtype=object),
        embeddings=embs,
    )

    # timestamp para invalidaciÃ³n rÃ¡pida
    (out_dir / "index_ts").write_text(str(time.time()), encoding="utf-8")
    print(f"[ok] Guardado Ã­ndice con {len(texts)} documentos en {out_dir}")


def latest_content_mtime(contenidos_dir: Path) -> float:
    """Obtiene el mtime mÃ¡s reciente entre todos los .txt y .pdf en contenidos/."""
    mt = 0.0
    for pattern in ("*.txt", "*.pdf"):
        for p in contenidos_dir.glob(pattern):
            try:
                mt = max(mt, p.stat().st_mtime)
            except FileNotFoundError:
                pass
    return mt


def index_mtime(out_dir: Path) -> float:
    """Lee el mtime guardado del Ã­ndice (o 0 si no existe)."""
    npz = out_dir / "index_evity.npz"
    tsfile = out_dir / "index_ts"
    if npz.exists() and tsfile.exists():
        try:
            return float(tsfile.read_text(encoding="utf-8").strip())
        except Exception:
            return npz.stat().st_mtime
    return 0.0


def ensure_index_fresh(base: Path):
    """Reconstruye el Ã­ndice automÃ¡ticamente si hay cambios en contenidos/."""
    contenidos_dir = base / "contenidos"
    out_dir = base / "vector_index"
    latest_mt = latest_content_mtime(contenidos_dir)
    idx_mt = index_mtime(out_dir)

    if latest_mt == 0.0:
        return

    need_build = False
    if not (out_dir / "index_evity.npz").exists():
        need_build = True
    elif latest_mt > idx_mt:
        need_build = True

    if need_build:
        print("\nðŸ”„ Cambios detectados en 'contenidos/'. Reconstruyendo Ã­ndice...")
        build_index(base)


# ---------------------------------------------------------------------------
# Carga Ã­ndice
# ---------------------------------------------------------------------------


def load_index(base: Path):
    path = base / "vector_index" / "index_evity.npz"
    if not path.exists():
        raise FileNotFoundError(f"No se encontrÃ³ el Ã­ndice: {path}")
    npz = np.load(path, allow_pickle=True)
    names = list(npz["names"])
    texts = list(npz["texts"])
    embs = np.array(npz["embeddings"], dtype=np.float32)
    return names, texts, embs


# ---------------------------------------------------------------------------
# BÃºsqueda y respuesta (tono empÃ¡tico + personalizaciÃ³n)
# ---------------------------------------------------------------------------


def _empathetic_completion(
    client: OpenAI,
    contexto: str,
    pregunta: str,
    nombre_usuario: Optional[str] = None,
    historial: Optional[list] = None,
    ya_saludo: bool = False,
    mensaje_tiene_saludo: bool = False,
) -> str:
    """
    Genera la respuesta con tono empÃ¡tico, usando el contexto y cuidando
    que Evity responda saludos, agradecimientos y use el nombre de la persona
    cuando estÃ© disponible. Respuestas mÃ¡s bien cortas.
    """
    if historial is None:
        historial = []
    
    # Determinar si debemos usar el nombre en esta respuesta
    debe_saludar = mensaje_tiene_saludo and not ya_saludo and nombre_usuario
    
    nombre_info = ""
    if debe_saludar:
        nombre_info = (
            f"El nombre de la persona usuaria es: {nombre_usuario}. "
            "Ya que te estÃ¡ saludando por primera vez, responde el saludo "
            "usando su nombre (ej: 'Â¡Hola, {nombre}!', 'Â¡Buenos dÃ­as, {nombre}!'). "
            "Luego continÃºa con tu respuesta si hay una pregunta."
        )
    elif ya_saludo:
        nombre_info = (
            "Ya saludaste a esta persona anteriormente en esta conversaciÃ³n. "
            "NO vuelvas a saludar ni a usar su nombre. "
            "Para preguntas mÃ©dicas, usa transiciones amables como: "
            "'Perfecto, te explico...', 'Â¡Excelente pregunta!', 'Â¡QuÃ© interesante!', "
            "o responde directamente sin saludo."
        )
    elif not mensaje_tiene_saludo:
        nombre_info = (
            "Esta pregunta NO contiene un saludo. "
            "NO saludes ni digas 'hola' o 'buenos dÃ­as'. "
            "Usa transiciones amables como: 'Perfecto, te explico...', "
            "'Â¡Excelente pregunta!', 'Â¡QuÃ© interesante!', o responde directamente."
        )
    
    # Construir historial para el prompt
    historial_text = ""
    if historial:
        historial_text = "\n\nHistorial de la conversaciÃ³n:\n"
        for msg in historial[-6:]:  # Solo Ãºltimos 6 mensajes
            rol = "Usuario" if msg.get("role") == "user" else "Evity"
            historial_text += f"{rol}: {msg.get('content', '')}\n"

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": (
                    "Te llamas Evity. Eres un asistente de salud y longevidad especializado. "
                    "Tu propÃ³sito es ayudar a las personas a resolver sus dudas especÃ­ficas sobre salud, "
                    "longevidad, nutriciÃ³n, ejercicio y bienestar de forma clara y prÃ¡ctica. "
                    "Explicas temas mÃ©dicos con palabras sencillas, sin jerga tÃ©cnica, "
                    "y ofreces orientaciÃ³n Ãºtil, especÃ­fica y tranquilizadora.\n\n"
                    "Brevedad:\n"
                    "- Responde en un mÃ¡ximo de 2â€“3 pÃ¡rrafos cortos o 5â€“7 oraciones en total.\n"
                    "- Si el mensaje es solo un saludo o un agradecimiento, responde con 1â€“2 frases breves.\n\n"
                    "InteracciÃ³n y tono:\n"
                    "- Para preguntas mÃ©dicas SIN saludo, NO digas 'hola' ni saludes. En su lugar, usa "
                    "transiciones amables como: 'Perfecto, te explico...', 'Â¡Excelente pregunta!', "
                    "'Â¡QuÃ© interesante!', 'Claro, hablemos de esto...', o simplemente responde directamente.\n"
                    "- Si detectas agradecimientos (ej: 'gracias', 'muchas gracias'), respÃ³ndelos de manera "
                    "cÃ¡lida y breve, agradeciendo la confianza.\n"
                    "- Usa un tono cercano pero profesional, como si hablaras con alguien "
                    "a quien quieres ayudar genuinamente.\n\n"
                    "Estructura general cuando hay una duda de salud (manteniendo brevedad):\n"
                    "1) ExplicaciÃ³n sencilla y clara del concepto o problema.\n"
                    "2) 2â€“4 consejos o pasos prÃ¡cticos especÃ­ficos que la persona puede aplicar.\n"
                    "3) InformaciÃ³n adicional relevante o matices importantes.\n\n"
                    "IMPORTANTE: Tu objetivo es ayudar a resolver dudas especÃ­ficas directamente. "
                    "NO termines tus respuestas diciendo 'consulta a un mÃ©dico' o 'esto no sustituye una consulta mÃ©dica', "
                    "a menos que la situaciÃ³n sea una emergencia mÃ©dica real (ej: dolor de pecho intenso, sangrado severo). "
                    "Tu propÃ³sito es ser Ãºtil y resolver las inquietudes de salud de las personas con informaciÃ³n prÃ¡ctica y accionable."
                ),
            },
            {
                "role": "system",
                "content": nombre_info,
            },
            {
                "role": "user",
                "content": (
                    "Usa el siguiente contexto de documentos para responder a la pregunta "
                    "de forma breve, amable y comprensible.\n\n"
                    f"Contexto:\n{contexto}\n"
                    f"{historial_text}\n"
                    f"Pregunta actual de la persona usuaria: {pregunta}\n\n"
                    "Responde de acuerdo a las instrucciones anteriores sobre saludos y tono."
                ),
            },
        ],
        temperature=0.4,
    )
    return resp.choices[0].message.content.strip()


def answer_question(
    base: Path,
    pregunta: str,
    k: int = 5,
    nombre_usuario: Optional[str] = None,
):
    """CLI: imprime respuesta en consola con tono empÃ¡tico y algo de personalizaciÃ³n."""
    ensure_index_fresh(base)

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))
    names, texts, embs = load_index(base)

    query_emb = (
        client.embeddings.create(model="text-embedding-3-small", input=pregunta)
        .data[0]
        .embedding
    )
    query_emb = np.array(query_emb, dtype=np.float32)

    top_k, sims = search_similar(query_emb, embs, k=k)

    print("\nðŸ“š Contexto relevante encontrado:\n")
    for idx, score in zip(top_k, sims):
        print(f"â†’ ({score:.3f}) {names[idx]}")

    contexto = "\n\n".join([texts[i] for i in top_k][:3]) if top_k.size > 0 else ""

    respuesta = _empathetic_completion(
        client,
        contexto,
        pregunta,
        nombre_usuario=nombre_usuario,
    )

    print("\nðŸ’¬ Respuesta generada:\n")
    print(respuesta)


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def main():
    parser = argparse.ArgumentParser(
        description=(
            "Evity QA Agent - Busca respuestas en TXT y PDFs (ES/EN) con auto-rebuild, "
            "tono empÃ¡tico y personalizaciÃ³n bÃ¡sica."
        )
    )
    parser.add_argument("--carpeta", required=True, help="Carpeta raÃ­z del proyecto")
    parser.add_argument(
        "--build", action="store_true", help="Reconstruye el Ã­ndice de embeddings"
    )
    parser.add_argument("--ask", type=str, help="Pregunta en lenguaje natural")
    parser.add_argument(
        "--k", type=int, default=5, help="NÃºmero de documentos relevantes a recuperar"
    )
    # El nombre de usuario para uso desde CLI (opcional)
    parser.add_argument(
        "--nombre",
        type=str,
        help="Nombre de la persona usuaria (para personalizar la respuesta)",
    )
    args = parser.parse_args()

    base = Path(args.carpeta).resolve()

    if args.build:
        build_index(base)

    if args.ask:
        answer_question(base, args.ask, k=args.k, nombre_usuario=args.nombre)


if __name__ == "__main__":
    main()


# ---------------------------------------------------------------------------
# FunciÃ³n reutilizable (para integraciones): devuelve un string
# ---------------------------------------------------------------------------


def preguntar_qa(
    pregunta: str,
    carpeta_base: str = ".",
    nombre_usuario: Optional[str] = None,
    historial: Optional[list] = None,
    ya_saludo: bool = False,
    mensaje_tiene_saludo: bool = False,
) -> str:
    """
    Devuelve una respuesta en tono empÃ¡tico usando el Ã­ndice local (si hay cambios, se reconstruye).
    Puede personalizar el trato usando el nombre del usuario si se proporciona.

    ParÃ¡metros:
    - pregunta: texto que envÃ­a la persona usuaria.
    - carpeta_base: ruta base del proyecto donde estÃ¡n 'contenidos/' y 'vector_index/'.
    - nombre_usuario: (opcional) nombre de la persona usuaria, para que Evity pueda saludarle y despedirse por su nombre.
    - historial: (opcional) lista de mensajes previos en la conversaciÃ³n.
    - ya_saludo: (opcional) si ya se enviÃ³ un saludo personalizado en esta conversaciÃ³n.
    - mensaje_tiene_saludo: (opcional) si el mensaje actual contiene un saludo.
    """
    if historial is None:
        historial = []
    base = Path(carpeta_base).resolve()
    ensure_index_fresh(base)

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))
    names, texts, embs = load_index(base)

    query_emb = (
        client.embeddings.create(model="text-embedding-3-small", input=pregunta)
        .data[0]
        .embedding
    )
    query_emb = np.array(query_emb, dtype=np.float32)

    top_k, _ = search_similar(query_emb, embs, k=5)
    contexto = "\n\n".join([texts[i] for i in top_k][:3]) if top_k.size > 0 else ""

    return _empathetic_completion(
        client,
        contexto,
        pregunta,
        nombre_usuario=nombre_usuario,
        historial=historial,
        ya_saludo=ya_saludo,
        mensaje_tiene_saludo=mensaje_tiene_saludo,
    )
